{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates a  HDF5-Database for a measurement day  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "\n",
    "''' Sn properties - Masses, Ratios, Cup Configurations, ... '''\n",
    "from sn_config import *\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "'''Classes for Reading in the Data and applying DS Inversion and Interference corr'''\n",
    "from nu_data_reduction import NU_data_read, normalisation, evaluation\n",
    "from parameter import Sn_meas_obj, spike_obj\n",
    "from dspike_formulas import *\n",
    "\n",
    "import pylab as plt\n",
    "from tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define path and parameter of measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Volumes/friebelm/PhD/NU Plasma/Measurements/2018-01-10/raw files/\"\n",
    "#path = \"/Users/marf/Desktop/PhD Temp/2017-12-05/\"\n",
    "DS_inv_hdf = pd.HDFStore(path+\"DS_inv_20171205.h5\")\n",
    "\n",
    "files_start = 9073\n",
    "files_end = 9248\n",
    "\n",
    "files_1 = range(files_start, files_end, 1)\n",
    "#files_1 = range(8808,8915,1) + range(8922,9061, 1)\n",
    "#files_1 = range(5552,5623,1) + range(5632,5729, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Isotopes used for Spike Inversion [[denominator isotope], nominator isotopes, .. ]\n",
    "spikeSn_ls = [[\"117\"],[\"118\", \"122\", \"124\"]]\n",
    "\n",
    "### spike and standard composition already defined in parameter.py ###\n",
    "\n",
    "# Dictionary with pure DS composition\n",
    "#spike_dict_117_122_meas = {\"118\": 0.0102951622334242, \"119\": 0.0090731830784630, \"120\" : 0.0338645175569080, \"122\" : 0.8068023572167630, \"124\" : 0.0114846216495240} # DS compositon 1ppm - 15.09.17\n",
    "\n",
    "# Dictionary with Standard composition\n",
    "#meas_dict = { \"118\" : 3.15777585757689, \"119\" : 1.11972979701600, \"120\" : 4.25050484928267, \"122\" : 0.60392336483548, \"124\" : 0.754692491283485} # mean of 1ppm NIST Std used for calib-15.09.17\n",
    "# Denominator isotope in ratios for DS & Std composition\n",
    "#meas_denom_117 = \"117\"\n",
    "\n",
    "# Load compositions in for DS inversion\n",
    "#Sn_meas_obj = load_ratio_dict(meas_dict, meas_denom_117)\n",
    "#spike_obj = load_ratio_dict(spike_dict_117_122_meas, meas_denom_117)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cup configuration\n",
    "#cup_config = cycle_Sb\n",
    "Sn_isotopes = [\"117\", \"118\", \"119\", \"120\", \"122\", \"124\"]\n",
    "cup_config = cycles_spike\n",
    "\n",
    "# Mass Range of cup configuration\n",
    "mass_range = cycle_Sn_spike_mass_range\n",
    "\n",
    "# Isotopes used for Interference correction\n",
    "corr_isotopes_2 = {\"Te\" : \"125\", \"Xe\" : \"129\"}\n",
    "#corr_isotopes_2 = {}\n",
    "#corr_isotopes_Sb = {\"Te\": \"125\"}\n",
    "\n",
    "denom_isotope = \"117\"\n",
    "Sn_monitor = [\"125\"]\n",
    "\n",
    "#def eval_iso_list(isotopes_list, denom_isotope, monitor_iso):\n",
    "#    isotope_den = denom_isotope\n",
    "#    isotopes_list.remove(isotope_den)\n",
    "#    for i in range(len(monitor_iso)):\n",
    "#        isotopes_list.append(monitor_iso[i])\n",
    "#    isotopes_list.sort()\n",
    "#   return isotopes_list\n",
    "\n",
    "#isotope_ls = eval_iso_list(Sn_isotopes, denom_isotope, Sn_monitor)\n",
    "  \n",
    "isotopes = [[\"118\", \"119\", \"120\", \"122\", \"124\"]]\n",
    "data_sample_column = [(i + \"/\" + denom_isotope) for i in isotopes[0]]  \n",
    "\n",
    "# columns for signal dataframe\n",
    "columns_1 = [\"cycle\", \"sample\", \"date\", \"H8 (1)\", \"H7 (1)\", \"H6 (1)\", \"H5 (1)\", \"H4 (1)\", \"H3 (1)\", \n",
    "             \"H2 (1)\", \"H1 (1)\", \"Ax (1)\", \"L1 (1)\", \"L2 (1)\", \"L3 (1)\", \"L4 (1)\"]\n",
    "\n",
    "# Amount of interations for estimation of instrumental and natural fractionation\n",
    "loop_nat = 3\n",
    "loop_ins = 6\n",
    "# assumended instrumental and natural fractionation\n",
    "start_nat = -0.1 #start_nat = -0.1; start_nat = 1000\n",
    "start_ins = 2 #start_ins = 2; start_ins = 0.001\n",
    "# isotope ratio used for calculation of instrumental and natural fractionation [denominator[x, y, z]] e.g. [117,[118, 122, 124]] x = 118/117\n",
    "inv_iso_ratio = 'x'\n",
    "\n",
    "# Mass fractionation law (\"exp\" or \"GPL\")\n",
    "law = \"exp\"\n",
    "\n",
    "# n for GPL\n",
    "n_GPL_ins = 0.3\n",
    "n_GPL_nat = 0.001\n",
    "\n",
    "# Interference_corr on the denominator isotope\n",
    "isotope_denom_corr = False\n",
    "# background correction\n",
    "blk_corr = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to evaluate data & extract metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def signals_raw(path, sample, cup_config, columns):\n",
    "    \n",
    "def metadata_df(data_read_obj, cycles):\n",
    "    # extract sample name from csv\n",
    "    sample_name = df.extract_metadata(sample, \"Sample Name\")\n",
    "    # extract date from csv\n",
    "    date = df.extract_metadata(sample, \"Date\")\n",
    "    # extract time from csv\n",
    "    starttime = df.extract_metadata(sample, \"Start Time\")\n",
    "    # extract Number of Analysis Cycles from csv\n",
    "    Num_of_Ana_Lines = df.extract_metadata(sample, \"Number of Analysis Cycles\")\n",
    "    # combine date and starttime\n",
    "    dateandtime = datetime.strptime(date+starttime, '%d/%m/%Y%H:%M')\n",
    "    \n",
    "    df_metadata = pd.DataFrame()\n",
    "    df_metadata[\"Cycle\"] = cycles\n",
    "    df_metadata[\"Filenumber\"] = sample\n",
    "    df_metadata[\"Sample\"] = sample_name\n",
    "    df_metadata[\"Date\"] = dateandtime\n",
    "    df_metadata[\"Num_of_Ana_Cyc\"] = Num_of_Ana_Lines\n",
    "    df_metadata = df_metadata[[\"Filenumber\", \"Sample\", \"Date\", \"Cycle\",  \"Num_of_Ana_Cyc\"]]\n",
    "    df_metadata = df_metadata.set_index(\"Cycle\")\n",
    "    \n",
    "    return df_metadata\n",
    "\n",
    "def signals_raw(data_read_obj, sample, cup_config, cycle=\"cycle1\"):\n",
    "    # create a dataframe with raw signals from csv datafile\n",
    "    signals_raw_all = df.data_read(sample)\n",
    "    signals_raw = pd.DataFrame()\n",
    "    for key in cup_config[cycle]:\n",
    "        if key in signals_raw_all.columns:\n",
    "            signals_raw[cup_config[cycle][key]] = signals_raw_all[key]\n",
    "    \n",
    "    signals_raw = signals_raw[sorted(signals_raw.columns)]\n",
    "    return signals_raw\n",
    "\n",
    "def signals_baseline(data_read_obj, sample, cup_config, cycle=\"zero1\"):\n",
    "    # create a dataframe with raw signals from csv datafile\n",
    "    signals_base_all = df.data_read(sample)\n",
    "    signals_base = pd.DataFrame()\n",
    "    for key in cup_config[cycle]:\n",
    "        if key in signals_base_all.columns:\n",
    "            signals_base[cup_config[cycle][key]] = signals_base_all[key]\n",
    "    \n",
    "    signals_base = signals_base[sorted(signals_base.columns)]\n",
    "    return signals_base\n",
    "\n",
    "def signals_baseline_corr(data_read_obj, sample, cup_config, cycle=\"cycle1\"):\n",
    "    # creates a dataframe with all zero corrected signals on the cups\n",
    "\n",
    "    ## perform baseline correction\n",
    "    df_zero = df.data_zero_corr(sample)\n",
    "    # create dataframe with baseline corrected data\n",
    "    df_zero_1 = pd.DataFrame(df_zero[cycle])\n",
    "    for key in cup_config[cycle]:\n",
    "        if key in df_zero_1.columns:\n",
    "            df_zero_1.rename(columns = {key : cup_config[cycle][key]}, inplace=True)\n",
    "    \n",
    "    df_zero_1 = df_zero_1[sorted(df_zero_1.columns)]\n",
    "    df_zero_1.index.name = \"Cycle\"\n",
    "    return df_zero_1\n",
    "    \n",
    "def bgd_before(data_read_obj, blk_ls, path, sample, cup_config):\n",
    "\n",
    "    cycles = range(1, len(data_read_obj.data_read(sample).index)+1)\n",
    "        \n",
    "    # arbitrary blank positions - check blk position before and after sample \n",
    "    blk1 = [item for item in blk_ls if item < sample]\n",
    "    \n",
    "    if not blk1:\n",
    "        return None\n",
    "    else:\n",
    "        blk1 = blk1[-1]\n",
    "        # Reads signals of Blank measurements\n",
    "        blk_1_obj = NU_data_read(path, blk1, cup_config)\n",
    "        # Substracts baselines of signals\n",
    "        df_bgd_1 = blk_1_obj.data_zero_corr(blk1)  \n",
    "\n",
    "        return df_bgd_1\n",
    "\n",
    "def bgd_after(data_read_obj, blk_ls, path, sample, cup_config):\n",
    "\n",
    "    cycles = range(1, len(data_read_obj.data_read(sample).index)+1) \n",
    "    \n",
    "    # arbitrary blank positions - check blk position before and after sample \n",
    "    blk2 = [item for item in blk_ls if item > sample]\n",
    "    \n",
    "    if not blk2:\n",
    "        return None\n",
    "    else:\n",
    "        blk2 = blk2[0]\n",
    "        # Reads signals of Blank measurements\n",
    "        blk_2_obj = NU_data_read(path, blk2, cup_config)\n",
    "        # Substracts baselines of signals\n",
    "        df_bgd_2 = blk_2_obj.data_zero_corr(blk2)   \n",
    "    \n",
    "        return df_bgd_2\n",
    "\n",
    "def raw_ratios(data_eval_obj):\n",
    "    #Get raw ratios\n",
    "    data_sample = data_eval_obj.raw_ratios(denom_isotope)\n",
    "    # raw ratios from dataframe in dictionary for data handling\n",
    "    data_sample = pd.DataFrame.from_dict(data_sample, orient = 'index')\n",
    "\n",
    "    return data_sample\n",
    "\n",
    "def df_plus_metadata(df_meta, df_values):\n",
    "    df_values = pd.concat([df_meta, df_values], axis=1, ignore_index=False).reset_index()\n",
    "    df_values = df_values.set_index(\"Date\")\n",
    "    \n",
    "    return df_values\n",
    "\n",
    "def add_epsilon_Q(df_values):\n",
    "    #Calculation of eSn, fsam, fspike, Q\n",
    "    \n",
    "    \n",
    "    df_values[\"eSn_118_117\"] = ((df_values[\"Nr2:x\"]/df_values[\"n0:x\"])-1)*10000\n",
    "    df_values[\"eSn_122_117\"] = ((df_values[\"Nr2:y\"]/df_values[\"n0:y\"])-1)*10000\n",
    "    df_values[\"eSn_124_117\"] = ((df_values[\"Nr2:z\"]/df_values[\"n0:z\"])-1)*10000\n",
    "    df_values[\"eSn_122_118\"] = (((df_values[\"Nr2:y\"]/df_values[\"Nr2:x\"])/(df_values[\"n0:y\"]/df_values[\"n0:x\"]))-1)*10000\n",
    "    df_values[\"eSn_124_118\"] = (((df_values[\"Nr2:z\"]/df_values[\"Nr2:x\"])/(df_values[\"n0:z\"]/df_values[\"n0:x\"]))-1)*10000\n",
    "    df_values[\"fsam\"] = 1/((df_values[\"Mr2.5:x\"]*0.07669971 - 0.24220048)/(0.00525699 - df_values[\"Mr2.5:x\"]*0.51062751)+1)\n",
    "    df_values[\"fspike\"] = 1-df_values[\"fsam\"]\n",
    "    df_values[\"Q\"] = df_values[\"fsam\"]/df_values[\"fspike\"]\n",
    "    \n",
    "    return df_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database files for raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Database for raw data\n",
    "\n",
    "blk_ls = []\n",
    "sample_ls = []\n",
    "\n",
    "for sample in files_1:\n",
    "    # read data from csv file\n",
    "    df = NU_data_read(path, sample, cup_config)\n",
    "    sample_name = df.extract_metadata(sample, \"Sample Name\")\n",
    "\n",
    "    # determine number of cycles of file\n",
    "    cycles = range(1, len(df.data_read(sample).index)+1)\n",
    "\n",
    "    df_meta = metadata_df(df, cycles)\n",
    "\n",
    "    ### baseline corrected signals\n",
    "    df_signals = signals_raw(df, sample, cup_config)\n",
    "    df_signals = df_plus_metadata(df_meta, df_signals)\n",
    "    DS_inv_hdf.append('/raw_data/signals_raw', df_signals, min_itemsize = {\"Sample\": 30})\n",
    "    \n",
    "    df_baseline = signals_baseline(df, sample, cup_config)\n",
    "    df_baseline = df_plus_metadata(df_meta, df_baseline)\n",
    "    DS_inv_hdf.append('/raw_data/baselines', df_baseline, min_itemsize = {\"Sample\": 30})\n",
    "    \n",
    "    df_signals_zero = signals_baseline_corr(df, sample, cup_config)\n",
    "    df_signals_zero = df_plus_metadata(df_meta, df_signals_zero)\n",
    "    DS_inv_hdf.append('/raw_data/signals_zero', df_signals_zero, min_itemsize = {\"Sample\": 30})\n",
    "    \n",
    "    \n",
    "    # create list with sample file numbers of sample measurements and blank measurements\n",
    "    if sample_name == \"blank sol\" or sample_name == \"wash\" or sample_name == \"wash clean\" or sample_name == \"Teflon blk\":\n",
    "        blk_ls.append(sample)\n",
    "    elif sample_name == \"SQ\" or sample_name == \"Teflon blk\" or sample_name == \"4ml HDPE blank\":\n",
    "        None\n",
    "    else:\n",
    "        sample_ls.append(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database for DS_inv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to loop over for different interference corrections\n",
    "interf_corr_dict = {\"No_interference_corr\" : {}, \"Te125_Xe129_corr\" : {\"Te\" : \"125\", \"Xe\" : \"129\"}, \"Te126_Xe129_corr\" : {\"Te\" : \"126\", \"Xe\" : \"129\"}}\n",
    "\n",
    "# List to loop over for bgd_corr and no_bgd_cor\n",
    "bgd_corr_ls = [True, False]\n",
    "\n",
    "# Outlier rejection un-/activated for background data\n",
    "bgd_outlier_rej = True\n",
    "\n",
    "for sample in sample_ls:\n",
    "    \n",
    "    # read data from csv file\n",
    "    data_raw_obj = NU_data_read(path, sample, cup_config)\n",
    "    # determine number of cycles of file\n",
    "    cycles = range(1, len(data_raw_obj.data_read(sample).index)+1)\n",
    "    # baseline corrected dataframe\n",
    "    df_zero = data_raw_obj.data_zero_corr(sample)\n",
    "    # dataframe with metadata(e.g. Filenumber, Date, etc.)\n",
    "    df_meta = metadata_df(data_raw_obj, cycles)\n",
    "\n",
    "    for bgd_corr in bgd_corr_ls:\n",
    "        # iterate over type of interference correction\n",
    "          \n",
    "        for key in interf_corr_dict:\n",
    "\n",
    "            # create Data_evaluation object\n",
    "            data_eval_obj = evaluation(df_zero, cycles, isotopes, cup_config, database, mass_range, interf_corr_dict[key], denom_corr_ratio, law, n_GPL_ins)\n",
    "\n",
    "            # perform on peak background correction\n",
    "            if bgd_corr == True:\n",
    "                bgd1 = bgd_before(df, blk_ls, path, sample, cup_config)\n",
    "                bgd2 = bgd_after(df, blk_ls, path, sample, cup_config)\n",
    "                if bgd_outlier_rej == True:\n",
    "                    bgd1 = data_eval_obj.mad_outlier_rejection_dict(bgd1)\n",
    "                    bgd2 = data_eval_obj.mad_outlier_rejection_dict(bgd2)\n",
    "                    bgd_method = 'bgd_outlier_corr'\n",
    "                else:\n",
    "                    bgd_method = 'bgd_no_outlier_corr'\n",
    "                \n",
    "                df_bgd_corr = pd.DataFrame(df_zero['cycle1']) - pd.DataFrame(data_eval_obj.data_bgd_corr_2(bgd1, bgd2)['cycle1'])\n",
    "                for cup in cup_config['cycle1']:\n",
    "                    if cup in df_bgd_corr.columns:\n",
    "                        df_bgd_corr.rename(columns = {cup : cup_config['cycle1'][cup]}, inplace=True)\n",
    "    \n",
    "                df_bgd_corr = df_bgd_corr[sorted(df_bgd_corr.columns)]\n",
    "            else:\n",
    "                df_bgd_corr = pd.DataFrame()\n",
    "                bgd_method = 'no_bgd_corr'\n",
    "                \n",
    "            # get raw ratios for DS-inversion\n",
    "            df_raw_ratios = raw_ratios(data_eval_obj)\n",
    "\n",
    "            # First DS-inversion without interference correction\n",
    "            data_spike_obj_1 = calc_dspike_sample(Sn_meas_obj, df_raw_ratios, spike_obj, Sn_masses, spikeSn_ls , denom_isotope, law, n_GPL_ins, n_GPL_nat)\n",
    "            data_spike_calc_1 = data_spike_obj_1.dspike_corr(loop_nat, loop_ins, start_nat, start_ins, inv_iso_ratio)\n",
    "      \n",
    "            # Beta for interference correction\n",
    "            beta = data_spike_calc_1[\"frac_ins_x2.5\"]\n",
    "\n",
    "            # Interference correction\n",
    "            spike_corr = data_eval_obj.norm_beta_to_raw(\"Sn\", spikeSn_ls[0][0], beta)\n",
    "\n",
    "            # interference corrected raw ratios\n",
    "            df_corr_raw_ratios = pd.DataFrame.from_dict(spike_corr, orient = 'index')\n",
    "    \n",
    "            # amount of Interference correction\n",
    "            df_inter_corr = df_raw_ratios - df_corr_raw_ratios\n",
    "            df_raw_ratios = df_raw_ratios[sorted(df_raw_ratios.columns)]\n",
    "            df_raw_ratios.columns = data_sample_column\n",
    "            df_inter_corr = df_inter_corr[sorted(df_inter_corr.columns)]\n",
    "            df_inter_corr.columns = data_sample_column\n",
    "            \n",
    "            # Second DS-inversion with interference corr raw_ratios\n",
    "            data_spike_obj_2 = calc_dspike_sample(Sn_meas_obj, df_corr_raw_ratios, spike_obj, Sn_masses, spikeSn_ls , denom_isotope, law, n_GPL_ins, n_GPL_nat)\n",
    "            data_spike_calc_2 = data_spike_obj_2.dspike_corr(loop_nat, loop_ins, start_nat, start_ins, inv_iso_ratio)\n",
    "\n",
    "            df_raw_ratios = df_plus_metadata(df_meta, df_raw_ratios)\n",
    "            \n",
    "            df_corr_raw_ratios = df_plus_metadata(df_meta, df_corr_raw_ratios)\n",
    "            DS_inv_hdf.append('/evaluation/'+bgd_method +'/' +key +'/df_raw_ratios_af_interf_corr', df_corr_raw_ratios, min_itemsize = {\"Sample\": 50})\n",
    "            \n",
    "            data_spike_calc_1 = df_plus_metadata(metadata_df(data_raw_obj, data_spike_calc_1.index.values), data_spike_calc_1)\n",
    "            data_spike_calc_1[\"Cycle\"] = data_spike_calc_1[\"Cycle\"] + 1\n",
    "            data_spike_calc_1 = add_epsilon_Q(data_spike_calc_1)\n",
    "            DS_inv_hdf.append('/evaluation/'+bgd_method +'/' +key +'/df_DS_inv_bf_interf_corr', data_spike_calc_1, min_itemsize = {\"Sample\": 50})\n",
    "            \n",
    "            df_inter_corr = df_plus_metadata(df_meta, df_inter_corr) \n",
    "            DS_inv_hdf.append('/evaluation/'+bgd_method +'/' +key +'/df_amount_interf_corr', df_inter_corr, min_itemsize = {\"Sample\": 50})\n",
    "            \n",
    "            data_spike_calc_2 = df_plus_metadata(metadata_df(data_raw_obj, data_spike_calc_2.index.values), data_spike_calc_2)\n",
    "            data_spike_calc_2[\"Cycle\"] = data_spike_calc_2[\"Cycle\"] + 1\n",
    "            data_spike_calc_2 = add_epsilon_Q(data_spike_calc_2)\n",
    "            DS_inv_hdf.append('/evaluation/'+bgd_method +'/' +key +'/df_DS_inv_af_interf_corr', data_spike_calc_2, min_itemsize = {\"Sample\": 50})\n",
    "\n",
    "        DS_inv_hdf.append('/evaluation/'+bgd_method +'/df_raw_ratios', df_raw_ratios, min_itemsize = {\"Sample\": 50})\n",
    "        df_bgd_corr = df_plus_metadata(df_meta, df_bgd_corr)\n",
    "        DS_inv_hdf.append('/evaluation/'+bgd_method +'/df_bgd_corr' , df_bgd_corr, min_itemsize = {\"Sample\": 50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close Database after adding all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DS_inv_hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DS_inv_hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
